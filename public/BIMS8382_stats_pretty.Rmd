---
title: "Everyday Statistics"
author: "Marieke Jones"
date: "4/16/2020"
output:
  prettydoc::html_pretty:
    theme: leonids
    highlight: github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, warning = FALSE, message = FALSE)
```

# Today's files

 Before we get started, download the zip file containing the filled-in script, the skeleton script, printable version of our work for today, the project file, and the data file (csv).

Unzip the file and then click on the project file to begin

[Download the Files for Today's Class](https://github.com/HSL-Data/BIMS_8382_Web/blob/master/public/zips/stats_files.zip?raw=true)

# Goal

The goal of this session is to teach you how to conduct basic statistical analyses in R. We will cover descriptive statistics, exploratory data analysis, t-tests, ANOVA, and methods for discrete variable analysis. We do not have time to cover testing the assumptions and the underlying theory or mathematics.

# Setting up

(Install and) Load the packages we will need today.

```{r}
# install.packages("inspectdf")
# install.packages("broom")

library(tidyverse)
library(inspectdf)
library(broom)
```

Next, let's load in the `WolfHormoneData.csv` dataset. 

These data are hormone concentrations from 43 female maned wolves at the Smithsonian National Zoo -- part of my dissertation. We were interested to see whether the housing status of the female impacts the reproductive hormone concentrations.

We will use the `read_csv()` function from the tidyverse to get our data into R. Once we've loaded it, we can type the name of the object itself (`hormone`) to see it printed to the screen.

```{r}
hormone <- read_csv("data/WolfHormoneData.csv")
```
    
Let's get a feel for this new dataset to familiarize ourselves with its structure

```{r}
glimpse(hormone)
names(hormone)
summary(hormone)
```

# Descriptive statistics

We can either calculate descriptive statistics using the base R syntax with the `$` operator, e.g., `mydataframe$specificVariable` or using dplyr's `summarize()` function

### Descriptive statistics for categorical variable

Printing out all the **`Status`** values in the data will not be useful. It will print 1000 rows and then stop, trying to warn us that we didn't really want to do that.

```{r, results='hide'}
# Display all Status values
hormone$Status
```

# EXERCISE 1

A. See what the `unique` values of Status are using base R 
B. Use a dplyr pipeline to retrieve the same information

<details><summary>Click here for the answers </summary>

```{r}
# Get the unique values of Status
unique(hormone$Status)

# Do the same thing the dplyr way
hormone %>% distinct(Status)
```

</details>

```{r}
#create a table of frequencies
table(hormone$Status) #base r
hormone %>% count(Status) #this returns a tibble

#2-factor table or cross tabulation
table(hormone$Status, hormone$Animal)
hormone %>% count(Status, Animal)
```

### Descriptive statistics for continuous variables

Now let's calculate some descriptive stats for the **`PgConc`** variable.

```{r}
# measures of the middle: mean, median
mean(hormone$PgConc)
median(hormone$PgConc)

# measures of overall spread
range(hormone$PgConc)
sd(hormone$PgConc)
# the Interquartile Range (difference between 75th and 25th quartiles)
IQR(hormone$PgConc)

# quartiles (points that divide data into quarters)
quantile(hormone$PgConc)

# 90th percentile of expression
quantile(hormone$PgConc, probs = .90)

# a distributional summary
summary(hormone$PgConc)
```

What are your conclusions about this variable?

Recall the following about the mean and the median:
- The median is the middle of the sorted data
- The mean is the "balance point" of the data
- Symmetric data have similar means and medians

You can calculate these same descriptive statistics using dplyr, but remember, this returns a single-row, single-column tibble, _not_ a single scalar value like the above. This is only really useful in the context of grouping and summarizing. 

```{r}
# Compute the median PgConc for Single wolves
hormone %>% 
  filter(Status == "Single") %>%
  summarize(median(PgConc))

# Now for each Status
hormone %>% 
  group_by(Status) %>%
  summarize(med = median(PgConc))

# let's order that output
hormone %>% 
  group_by(Status) %>%
  summarize(med = median(PgConc)) %>%
  arrange(med)
```

# Stats with continuous variables

A very well-known group of basic statistical tests is called the t-test. A t-test is used when we want to know the difference between 2 group means.

T-tests assume a few things about the data:

1. Data were randomly sampled from their populations
2. two groups are independent from one another
3. groups were sampled from populations with normal distributions (symmetrical, bell-shaped)
4. variance is equal for the two populations

We do not have time here to talk through what to do if each of these assumptions is not met, so please reach out to us for help with your own data. But to help a little here is a brief flowchart:

- Random Sampling -- if not met, no statistics will help you
- Independent Samples -- if not met, use a paired t-test
- Normality -- if not met, use a Wilcoxon-Mann-Whitney U test
- Equal variance -- if not met, use a Welch's t-test
- If independent samples and normality are both not met -- use Wilcoxon signed rank test

### T-tests

Let's do a two-sample t-test to assess the _difference in means between two groups_. The function for a t-test is `t.test()`. See the help using `?t.test`.

One of the hypotheses I was testing with these data was whether females housed with other females experience a phenomenon called reproductive suppression. We'll investigate whether the females who were housed with other females show the same progesterone concentration in the luteal phase of their cycle as females housed singly.

First let's create a dataset that just contains the two statuses we are interested in

```{r}
twostat <- hormone %>%
  filter(Status == "WithFemales" | Status == "Single")
```

Next, let's create some exploratory plots to answer our questions about the assumptions. I like density plots colored by group. I think these help you see the distribution of the variable to assess equal variance. This plot also informs you somewhat on normality, and helps you see if there is a noticeable difference in groups.

```{r}
ggplot(twostat, aes(PgConc, fill = Status)) + geom_density(alpha = 0.5)
```

Whoa! These distributions are definitely not normally distributed, but I know that these data are typically log transformed...

# EXERCISE 2

Create a similar plot to above but with the log-transformed Pg concentration. Based on this plot, do these distributions meet the assumptions of normality and equal variance?

<details><summary>Click here for the answers </summary>

```{r}
ggplot(twostat, aes(LogPg, fill = Status)) + geom_density(alpha = .5)
```

</details>

First we make sure that a t-test is the correct type of analysis. A t-test tests the difference in 2 means - yes that is what we want to do. Next we need to decide what type of t-test we need to perform by thinking through the assumptions. Domain specific knowledge and exploratory data analyses will help here.

Random sampling -- YES, we did our best to sample a random subset of maned wolves housed in zoos

Independent samples -- YES (Singly housed wolves and females housed with other females are different individuals - no pairing). It could be a paired t-test if each single female was matched to a female-housed wolf based on age or another factor.

Equal variance. Also called homoscedasticity of variances.
?? we could think about the populations of single and female-housed wolves and what we know about how those statuses impact the variability of progesterone...if you were a wolf expert. You probably don't feel an expert in these variables, so let's allow the EDA plots to help. Looking on the width of the bases of the density plots, it seems that single females have larger variability than female-housed females.

Normality -- Our density plots don't look too normal, but there are better ways to assess this.

Normality can be assessed graphically or via hypothesis tests. There are pros and cons to either approach. 

Graphically, we could look at a histogram, boxplot, or a more specialized plot to assess normality called a QQ plot (quantile-quantile plot or quantile comparison plot or normal probability plot). A QQ plot graphs the expected data value given a normal distribution on the X axis against the observed data value on the y axis. If the data is normally distributed, we should see a 1:1 ratio between the expected values and the observed values. Let's have a look for LogPg:

```{r}
ggplot(twostat, aes(sample = LogPg)) + 
  geom_qq() +
  geom_qq_line() +
  facet_wrap(~Status) 
# actually not too bad
```

Learning what is a normal QQ plot looks like is a process.

Certain fields love hypothesis tests of normality and sometimes reviewers will specifically request one. There is a theoretical problem with trying to _prove_ a null hypothesis and they are known to reject the null when sample sizes are large. My best advice is to use your brain, subject matter expertise, and graphical assessments as much as possible, but in case you are forced to do a hypothesis test for normality check out `shapiro.test()`, since it seems to be the least awful choice (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3693611/).

Ok. We've checked our assumptions and are ready to perform a two-sample, unpooled variance t-test

```{r}
t.test(LogPg ~ Status, data=twostat)

# what if we had assumed equal variances
t.test(LogPg ~ Status, var.equal = TRUE, data=twostat)
```

In either case, we conclude that the Status groups are different in terms of their LogPg. We also can see a 95% confidence interval for the difference, meaning that we are 95% confident that the difference in *population* LogPg is between 1.29 and 1.74 (Welch's) or between 1.27 and 1.77 (pooled).

What if we felt uncomfortable with the assumption of normality? t-tests are robust to departures in normality, so in this case I think a t-test is the best option, but for demonstration purposes, the non-parameteric alternative is the Wilcoxon-Mann-Whitney U test.

```{r}
wilcox.test(LogPg ~ Status, data=twostat)
```

Notice that this output does not provide confidence intervals or summary statistics, but we can add a confidence interval for the difference and calculate the summary statistics ourselves.

```{r}
# with confidence interval for the pseudo-median difference
wilcox.test(LogPg ~ Status, data=twostat, conf.int = TRUE)

# calculate means and medians by group
twostat %>%
  group_by(Status) %>%
  summarize(med = median(LogPg),
            mean = mean(LogPg))
```

>**A note on one-tailed versus two-tailed tests:** A two-tailed test is usually more appropriate. The hypothesis you're testing here is spelled out in the results ("alternative hypothesis: true difference in means is not equal to 0"). If the p-value is very low, you can reject the null hypothesis that there's no difference in means. Because you may not know _a priori_ whether the difference in means will be positive or negative, you want to do the two-tailed test. However, if we _only_ wanted to test a very specific directionality of effect, we could use a one-tailed test and specify which direction we expect. This is more powerful if we "get it right", but much less powerful for the opposite effect. The p-value of a one-tailed test will be half of that of a two-tailed hypothesis test. BUT again, the **two-tailed test is almost always conducted** to be conservative.

>**A note on paired versus unpaired t-tests:** The t-tests we performed here were unpaired tests. Females housed with females and females housed alone are different individuals and their observations are not related. In these cases, an _unpaired_ test is appropriate. An alternative design might be when data is derived from samples measured at two different time points or locations, e.g., before versus after treatment, left versus right hand, etc. In this case, a _**paired t-test**_ would be more appropriate. A paired test takes into consideration the intra and inter-subject variability, and is more powerful than the unpaired test. There is a paired = TRUE option for both the t-test and the Wilcoxon test.

# ANOVA and Linear models

> Analysis of variance (ANOVA) and linear modeling are complex topics that deserve an entire semester dedicated to theory, design, and interpretation. Luckily, next week we will be focused on linear modeling. What follows is a necessary over-simplification with more focus on implementation, and less on theory and design.

Where t-tests and their nonparametric substitutes are used for assessing the differences in means between two groups, ANOVA is used to assess the significance of differences in means between multiple groups. In fact, a t-test is just a specific case of ANOVA when you only have two groups. And both t-tests and ANOVA are just specific cases of linear regression, where you're trying to fit a model describing how a continuous outcome (e.g., LogPg) changes with some predictor variable (e.g., Status, sex, etc.). The distinction is that ANOVA has a categorical predictor while linear modeling has a continuous predictor.

Let's use ANOVA to test the differences in LogPg between all five statuses for our female wolves.

First let's see how many observations of each we have.

```{r}
hormone %>% count(Status)
```

ANOVA prefers balanced sample sizes. If you can choose to design your study with balanced sample sizes, ANOVA will perform better (better power, higher probability of equal variances assumption being met, etc). These data are not too imbalanced but definitely could be better.

# EXERCISE 3

Run a grouped summary to see the mean LogPg for each Status

<details><summary>Click here for the answers </summary>

```{r}
hormone %>%
  group_by(Status) %>%
  summarize(mean = mean(LogPg))
```

</details>

Ok, so PregSuccess and PregLost seem higher than the other Statuses.

Now that we understand these data a bit, let's proceed to fit the model. In R fitting an ANOVA model is done by fitting a regression model, called a linear model, underscoring our knowledge that ANOVA and linear models are actually the same thing.

```{r}
fit <- lm(LogPg ~ Status, data=hormone)
```

To see the ANOVA output we call the `anova()` function on the linear model fit object

```{r}
anova(fit)
```

The F-test on the ANOVA table tells us that there _is_ a significant difference in mean LogPg between the housing statuses (p=$2.2 \times 10^{-16}$). 

For more details, let's take a look at the linear model output

>**A note on dummy coding:** If you have a $k$-level factor, R creates $k-1$ dummy variables, or indicator variables, by default, using the alphabetically first level as the reference group. For example, there are 5 levels of Status in our dataset. R creates a dummy variable called "StatusSingle" that's **1** if the wolf is Single and **0** if the wolf is in any other Status group. The linear model is saying for every unit increase in StatusSingle, i.e., going from NotPreg to Single, results in a 1.78-unit decrease in LogPg. You can change the ordering of the factors to change the interpretation of the model (e.g., treating PregSuccess as baseline). We'll do this in the next section.

```{r}
summary(fit)
```

Interpretation:
- the mean LogPg for the NotPreg females = 2.96
- the PregLost females have a mean logPg 0.53 higher than that of NotPreg. That difference is statistically significant
- the PregSuccess females have a mean logPg 0.66 higher than that of NotPreg. That difference is statistically significant
- the Single females have a mean logPg 1.79 lower than that of NotPreg. That difference is statistically significant
- the WithFemale females have a mean logPg 3.31 lower than that of NotPreg. That difference is statistically significant

The bottom of the output has the ANOVA information and model summary statistics.

Because the default handling of categorical variables is to treat the alphabetical first level as the baseline, "NotPreg" females were treated the baseline group (the intercept row) and the coefficients for the other groups describe how those groups' mean LogPg differs from that of the NotPreg females 

What if we wanted PregSuccess to be the reference category, followed by PregLost, then NotPreg, Single, then WithFemales? Let's use `mutate()` and `factor()` to change the factor levels.

```{r}
hormone <- hormone %>%
  mutate(Status = factor(Status, levels = c("PregSuccess", "PregLost", "NotPreg", "Single", "WithFemales")))
```

Re-run the model to see how re-leveling the variable changes the output

```{r}
fit <- lm(LogPg ~ Status, data = hormone)
anova(fit)
```

Same F statistic and p-value overall. Let's have a look at the summary

```{r}
summary(fit)
```

Now PregSuccess is the reference group and the comparisons are to that Status

So far, we have been looking at model output as a large paragraph. If you need to do something downstream with the output from these models, the `tidy()` and `glance()` functions in the broom package may help. For example, these functions can help tremendously if you'd like to output the results of a model into a table. These functions work for t-test, wilcox.test, ANOVA, and linear models (and maybe other types of tests too).

```{r}
# coefficients section
tidy(fit)

# model summary section
glance(fit)
```

If you wanted to remain in the ANOVA framework, you can run the typical post-hoc ANOVA procedures on the fit object. For example, the `TukeyHSD()` function will run [_Tukey's test_](https://en.wikipedia.org/wiki/Tukey%27s_range_test). Tukey's test computes all pairwise mean difference calculation, comparing each group to each other group, identifying any difference between two groups that's greater than the standard error, while controlling the type I error for all multiple comparisons. First run `aov()` (**not** `anova()`) on the fitted linear model object, then run `TukeyHSD()` on the resulting analysis of variance fit.

```{r}
TukeyHSD(aov(fit))
tidy(TukeyHSD(aov(fit)))

plot(TukeyHSD(aov(fit)))
```

This shows that there isn't much of a difference between PregLost and PregSuccess, but that both of these differ significantly from the other statuses. All other statuses are also significantly different from each other.

Finally, let's visualize the differences between these groups.

```{r}
# plot results
hormone %>%
  ggplot(aes(Status, LogPg)) + geom_boxplot()

# a fancier plot using a grouped summary
hormsum <- hormone %>%
  group_by(Status) %>%
  summarise(mean = mean(LogPg),
            sd = sd(LogPg),
            lower = mean - sd,
            upper = mean + sd)

ggplot() + 
  geom_jitter(data = hormone, aes(x= Status, 
                             y = LogPg, 
                             col = Status), 
              width = .1, alpha = .05) + 
  geom_pointrange(data = hormsum, aes(x = Status, 
                                    y = mean, 
                                    ymin = lower, 
                                    ymax = upper, 
                                    col = Status), 
                  size = 1, fatten = 2) + 
  theme_classic() +
  labs(y = "Log-transformed Progesterone",
       x = "",
       col = "Status")
```

# Discrete variable Statistics

Until now we've only discussed analyzing _continuous_ outcomes (dependent variables). We've tested for differences in means between two groups with t-tests, differences among means between _n_ groups with ANOVA, and more general relationships using linear regression. In all of these cases, the dependent variable, i.e., the outcome, or $Y$ variable, was _continuous_. What if our outcome variable is _discrete_, e.g., "Yes/No", "Pregnant/NotPregnant", etc.? Here we use a different set of procedures for assessing significant associations.

So far we have covered:
1. T-tests -- analyzing differences in one continuous variable between 2 groups
2. ANOVA -- analyzing differences in one continuous variable between 3+ groups
3. LM -- Next week we will go through analyzing the impact of a continuous variable on another continuous variable, but here we did an introduction to the functions we will need.

### Test of proportion

One of the most basic questions we could have is what proportion of my data...

The wolf dataset does not lend itself well to discrete variable statistics, so let's load another dataset. This is the diamonds dataset that comes with the ggplot2 package.

```{r}
diamonds
```

Total number of diamonds in this dataset is 53,940. Let's add a variable to diamonds that categorizes the price into Very Inexpensive -> Very Expensive (in that order).

```{r}
quantile(diamonds$price, probs = c(.33, .66))

diamonds <- diamonds %>%
  mutate(priceCat = case_when(
    price < 1228 ~ "Inexpensive",
    price >= 1228 & price < 4221 ~ "Average",
    price >= 4221 ~ "Expensive"
    )) %>%
  mutate(priceCat = factor(priceCat, levels = c("Inexpensive", "Average", "Expensive")))
```

Let's create a sample of 500 diamonds from this dataset 

```{r}
diamonds %>%
  sample_n(500)
```

You'll see that the `sample_n()` function pulls a random 500 rows from this dataset. If we run it again, we will see a different 500 rows. 

Let's ask what proportion of my sample of diamonds have price > 10,000? We'll calculate the proportion and the raw number.

```{r}
diamonds %>%
  sample_n(500) %>%
  summarize(prop = mean(price > 10000),
            n = sum(price > 10000))
```

Run the above code again and we will see that a different sample of diamonds will lead to a different sample proportion. One of our questions should be what is the variability in that sample proportion?

```{r}
# generate a sample that is the same for all of us
set.seed(416)
samp <- diamonds %>%
  sample_n(500) 

# see the sample proportion and the raw number
samp %>%
  summarize(prop = mean(price > 10000),
            n = sum(price > 10000))
```

Let's create a confidence interval around this proportion

```{r}
prop.test(x = sum(samp$price > 10000), n = length(samp$price))
```

We are 95% confident that the population proportion of diamonds in the whole diamonds dataset > $10000 is between 0.067 and .119

The hypothesis test this function performs simply tests if the proportion = .5 (not helpful for us)

Let's increase the sample size and see what happens to our sample proportion and to our confidence interval

```{r}
# generate a sample that is the same for all of us
set.seed(416)
bigsamp <- diamonds %>%
  sample_n(5000)

# calculate the confidence interval
prop.test(x = sum(bigsamp$price > 10000), n = length(bigsamp$price))
```

Notice that the point estimate of our sample stayed similar but that the confidence interval shrank considerably. This is the wonderful effect of increasing your sample size!

Since we have access to our population (the diamonds dataset), let's see what the true proportion is

```{r}
mean(diamonds$price > 10000)
```

### Testing 2 proportions

Often we are curious to compare 2 proportions. For example, is there is difference in the proportion of diamonds priced over $15,000 where the cut is rated as "Ideal" versus where the cut is just rated as "Good"?

Let's use the `bigsamp` object we created above to test our question

```{r}
bigsamp %>%
  filter(cut == "Ideal" | cut == "Good") %>%
  group_by(cut) %>%
  summarise(prop = mean(price > 15000),
            n = sum(price > 15000),
            totN = n())
```

So 1.8% of "Good" diamonds are priced > $15000 while 2.75% of "Ideal" diamonds are that expensive. Let's do the test of proportions to see the confidence interval for the difference in proportions

The `prop.test()` function's first argument is called x and it is the number of "successes" for each group. The second argument, n, is the total number of homes in each group

```{r}
prop.test(x = c(8,54), n = c(440, 1961))
```

The difference between our sample proportions is likely between -0.025 and +0.006, a pretty large spread. Our two sample proportions are not statistically different.

Let's see what the population difference in proportions is

```{r}
diamonds %>%
  filter(cut == "Ideal" | cut == "Good") %>%
  group_by(cut) %>%
  summarise(prop = mean(price > 15000),
            n = sum(price > 15000),
            totN = n())
```

### Chi Square Contingency tables

Many times we are interested in how two discrete variables interact with each other. When ever you have count data with more than one variable, you should be thinking about contingency tables. This type of analysis is also called chi square test of independence.

The [`xtabs()`](http://stat.ethz.ch/R-manual/R-patched/library/stats/html/xtabs.html) function is useful for creating contingency tables from categorical variables. Let's create a cross tabulation showing hsdistrict and condition2 of the home, and assign it to an object called **`xt`**. After making the assignment, type the name of the object to view it.

```{r}
xt <- xtabs(~cut + priceCat, data = diamonds)
xt
```

Ok, so our data is a 5x3 table.

There are two useful functions, `addmargins()` and `prop.table()` that add more information or manipulate how the data is displayed. `addmargins()` adds totals for rows and columns. By default, `prop.table()` will divide the number of observations in each cell by the total. 

```{r}
# Add marginal totals
addmargins(xt)

# Get the proportional table
prop.table(xt)
#each cell divided by grand total
# That isn't really what we want
```

We want to to see what proportion of diamonds in each cut class fall into each price condition, so we want proportions by rows. Rows is the first dimension in R, so we specify `margin = 1`.

```{r}
# Calculate proportions over the first margin (rows)
prop.table(xt, margin=1)
```

Looks like as we increase the cut quality we increase the price (generally, but note that this trend does not follow for Ideal diamonds). Are these differences significant?

The chi-square test is used to assess the independence of these two factors (cut and priceCat). That is, if the null hypothesis that cut and priceCat are independent is true, then we would expect a proportionally equal number of diamonds within each cut condition across each price category

Let's do the test and see.

```{r}
chisq.test(xt)
```

The p-value is small suggesting that we found changes from a random distribution. 

The last thing we will do is to compare where the differences are by looking at our observed table, xt, and the values expected if there was an even distribution of conditions across hsdistricts.

```{r}
xt
chisq.test(xt)$expected
```

- We expected 531 Inexpensive Fair diamonds, and found fewer
- We expected 1618 Inexpensive Good diamonds, and found fewer
- We expected 3985 Inexpensive VeryGood diamonds, and found fewer
- We expected 4549 Inexpensive Premium diamonds, and found fewer
- We expected 7108 Inexpensive Ideal diamonds, and found **more**


- We expected 531 Average priced Fair diamonds, and found **more**
- We expected 1618 Average priced Good diamonds, and found fewer
- We expected 3988 Average priced VeryGood diamonds, and found the same
- We expected 4552 Average priced Premium diamonds, and found fewer
- We expected 7114 Average priced Ideal diamonds, and found fewer


- We expected 547 Expensive Fair diamonds, and found **more**
- We expected 1668 Expensive Good diamonds, and found **more**
- We expected 4108 Expensive VeryGood diamonds, and found **more**
- We expected 4689 Expensive Premium diamonds, and found **more**
- We expected 7327 Expensive Ideal diamonds, and found fewer

Hopefully this provided you with a brief introduction to basic statistics in R and leaves you wanting to learn more. Get in touch with us if you need help with your own research data!