---
title: "Predictive Analytics"
date: "March 18, 2019"
output: html_document
---

[Back to the main page](https://bims8382.netlify.com/)

## Setting up

I recommend creating a new Rproject in a directory for this workshop 

Add the datafile and script from our Collab page to the project directory:

* the h7n9.csv dataset
* PredictiveAnalytics_script.R

We will need the following packages so please make sure they are installed (using `install.packages("packagename")`) and loaded (using `library(packagename)`)

* tidyverse
* AmesHousing
* recipes
* caret
    
Later we will also need:

* randomForest
* kknn
* glmnet

```{r, setup, message=FALSE}
library(tidyverse)
library(AmesHousing)
library(recipes)
library(caret)

#later we will also need
library(randomForest)
library(kknn)
library(glmnet)
```

## Introduction to Predictive Analytics
**slides 1 - 8**

This class will provide hands-on instruction for using machine learning algorithms to predict an outcome, either continuous or discrete. 

We will cover feature extraction, preprocessing, missing value imputation, resampling, and a variety of modelling techniques to try to predict an outcome. We will assess the performance of predictive modeling procedures such as Random Forest, elastic net regularized regression, and k-nearest neighbors.

CEO and founder of Kaggle (machine learning competition), Anthony Goldbloom, says "There's the joke that 80 percent of data science is cleaning the data and 20 percent is complaining about cleaning the data"

So, we are going to do a little exploratory data analysis, feature extration, and missing value imputation before we attack machine learning (model training and testing)

## Introducing the Flu datset

As an example of a dataset with a binary outcome, we're going to use some epidemiological data collected during an influenza A (H7N9) outbreak in China in 2013.

The data we're using here is from the 2013 outbreak of [influenza A H7N9 in China](https://en.wikipedia.org/wiki/Influenza_A_virus_subtype_H7N9), analyzed by Kucharski et al., published in 2014.

> **Publication**: A. Kucharski, H. Mills, A. Pinsent, C. Fraser, M. Van Kerkhove, C. A. Donnelly, and S. Riley. 2014. Distinguishing between reservoir exposure and human-to-human transmission for emerging pathogens using case onset data. [_PLOS Currents Outbreaks_ (2014) Mar 7 Edition 1](http://doi.org/10.1371/currents.outbreaks.e1473d9bfc99d080ca242139a06c455f). 

> **Data**: Kucharski A, Mills HL, Pinsent A, Fraser C, Van Kerkhove M, Donnelly CA, Riley S (2014) Data from: Distinguishing between reservoir exposure and human-to-human transmission for emerging pathogens using case onset data. _Dryad Digital Repository_. https://doi.org/10.5061/dryad.2g43n.

The original data is available in the [outbreaks](https://cran.r-project.org/web/packages/outbreaks/index.html) package, which is a collection of several simulated and real outbreak datasets. The version we will use has been very slightly modified for use here. The analysis we'll do here is inspired by and modified from [Stephen Turner's Bioconnector workshop](https://github.com/stephenturner/workshops/blob/master/r-predictive-modeling.Rmd) and a [similar analysis by Shirin Glander](https://shiring.github.io/machine_learning/2016/11/27/flu_outcome_ML_post).

In the flu dataset, of 134 cases, 31 died, 46 recovered, and 57 cases do not have a recorded outcome. We'll develop models capable of predicting death or recovery for the unlabeled cases.

First we'll do some exploratory data analysis and data visualization to get an overall sense of the data we have. Notice that `read_csv()` correctly reads in the dates as date-formatted variables. Later on, when we run functions such as `median()` on a date variable, R will know how to handle those. You'll also notice that there are missing values throughout.

```{r, flu, message=FALSE}
flu <- read_csv("h7n9.csv")
flu
```

The **outcome** variable is the thing we're most interested in here -- it's the thing we want to eventually predict for the unknown cases. Let's look at the distribution of that outcome variable (Death, Recover or unknown (NA)), by **age**. We'll create a density distribution looking at age, with the fill of the distribution colored by outcome status.

```{r, flueda}
ggplot(flu, aes(age)) + 
  geom_density(aes(fill = outcome), alpha = 1/3)
```
Younger people recover more. People who die are older. NAs are in the middle which is good.

Let's look at the number of deaths, recoveries, and unknowns by sex. We'll create a bar plot showing the number of individuals with each outcome, separately by gender to see if gender matters.
```{r, flueda_gender}
ggplot(flu, aes(gender)) + 
  geom_bar(aes(fill = outcome), position = "dodge")
```
We can tell from this plot that:

* Sample sizes are low as soon as we start splitting the data. 
* We measured more men
* Proportionally more men recovered, but we see roughly the same pattern of outcomes by sex
  + sex is maybe not a good predictor of mortality  
* Number of unknowns is proportional by sex (this is good)


We can simply add an additional level detail. We will `facet_wrap` by province to see if there is an interaction between gender and province. 
```{r}
ggplot(flu, aes(gender)) + 
  geom_bar(aes(fill = outcome), position = "dodge") + 
  facet_wrap(~province)
```
Here we see super low sample sizes. Lots of unknowns in some provinces, fewer in other provinces.

Next we'll investigate whetehr there is an interaction between age and province
```{r}
ggplot(flu, aes(province, age)) + 
  geom_boxplot(aes(fill = outcome))
```
We see a definitive age effect in Jiangsu and Zhejiang, but other provinces don't show this effect.

Let's try a plot that is a bit more advanced. First, take a look at the data again. 
```{r, results='hide'}
flu
```
Notice how we have three different date variables: date of onset, hospitalization, and outcome.

I'd like to draw a plot showing the date on the x-axis with a line connecting the three points from onset, to hospitalization, to outcome (if known) for each patient (plot in original paper showed this).

Need to transform data from wide to long so that we have all the dates in one column and which timepoint it represents in a second column.

We'll use the `gather` function from the **tidyr** package to gather up all the `date_?` variables into a single column we'll call `key`, with the actual values being put into a new column called `date`.

```{r, pathplot}
#Want to show a connecting line between the three different date variables
flugather <- flu %>%
  gather(key, date, starts_with("date_")) %>%
  arrange(case_id)

ggplot(flugather, aes(date, age, color=outcome)) + 
  geom_point(aes(shape = key)) + 
  geom_path(aes(group = case_id)) + 
  facet_wrap(~province)
```
We see that there are lots of unconnected points, especially in Jiangsu and Zhejiang provinces, where one of these dates isn't known.

In Jiangsu, teal(recovery) are younger than red(death) paths

Lots of missing in Zhejiang

Shorter hospitalizations --> recovery?

Goal for this plot was to show you how to reshape the data using `gather` and how to join points using `geom_path()`

### Create a few more variables for the flu dataset

If date_hospitalization is missing, we assume that the person did not go to the hospital and that may be an important predictor of death / recovery. Let's create an indicator of whether someone was hospitalized or not. 

In order to use the other date variables, let's take the _dates_ of onset, hospitalization, and outcome, and transform these into _days_ between onset and hospitalization, and days between onset and outcome. The algorithms aren't going to look at one column then another to do this math -- we have to extract this feature ourselves.

```{r, daysto}
flu %>% 
  # if date_hosp is NOT missing, they DID go to the hospital
  mutate(hospital = !is.na(date_hospitalization)) %>%
  mutate(days_to_hosp = as.numeric(date_hospitalization - date_onset)) %>%
  mutate(days_to_outcome = as.numeric(date_outcome - date_onset))

#save new variables onto flu dataframe
flu <- flu %>% 
  # if date_hosp is NOT missing, they DID go to the hospital
  mutate(hospital = !is.na(date_hospitalization)) %>%
  mutate(days_to_hosp = as.numeric(date_hospitalization - date_onset)) %>%
  mutate(days_to_outcome = as.numeric(date_outcome - date_onset)) 
```

Additionally, it is possible that if the outcome is early, maybe it was before some public health intervention so if person got sick later, maybe there were more resources available. Let's make a variable called early outcome indicating whether someone had an early outcome (earlier than the median outcome date observed).
```{r, earlyonset}
flu %>%
  mutate(early_outcome = date_outcome < median(date_outcome, na.rm = TRUE)) %>%
  #take out any columns that start_with("date")
  select(-starts_with("date")) %>%
  mutate_if(is.logical, as.integer)

#save new variables onto dataframe
flu <- flu %>%
  mutate(early_outcome = date_outcome < median(date_outcome, na.rm = TRUE)) %>%
  #take out any columns that start_with("date")
  select(-starts_with("date")) %>%
  mutate_if(is.logical, as.integer)
```

## Introducing the AmesHousing dataset

For our example predicting a continuous outcome, we are going to use the Ames Iowa housing data. There are 2930 properties in the data. The Sale Price was recorded along with 81 predictors, including:

- Location (e.g. neighborhood) and lot information.
- House components (garage, fireplace, pool, porch, etc.).
- General assessments such as overall quality and condition.
- Number of bedrooms, baths, and so on

```{r, makeAmes}
ames <- make_ames() %>%
  select(-matches("Qu"))
```

## EXERCISE 1
**YOUR TURN**

1. Take about 10 minutes to create some exploratory plots to investigate relationships between predictors and Sale_Price in the Ames Housing data

```{r, include=FALSE}
#how many houses in each neighborhood?
ames %>%
  group_by(Neighborhood) %>%
  count() %>%
  ggplot(aes(x = Neighborhood, y = n)) + 
  geom_bar(stat = "identity") + 
  coord_flip()
# when modeling we are going to have to watch out for neighborhoods with hardly any data

#lots of houses called "average"
ames %>%
  ggplot(aes(x = Overall_Cond, y = Sale_Price)) + geom_boxplot()

# try with log sales price
ames %>%
  ggplot(aes(x = Overall_Cond, y = log(Sale_Price))) + geom_boxplot()

# investigate Longitude
ames %>% ggplot(aes(x = Longitude, y = Sale_Price)) + 
  geom_point(alpha = .5) + 
  scale_y_log10()
# odd, maybe we will want a non-linear term for Longitude

#SKIP
# try nonlinear term Longitude
ames %>% ggplot(aes(x = Longitude, y = Sale_Price)) + 
  geom_point(alpha = .5) + 
  scale_y_log10() +
  geom_smooth(method = "lm", 
    formula = y ~ splines::bs(x, 5), 
    se = FALSE)

# investigate Latitude
ggplot(ames, 
       aes(x = Latitude, y = Sale_Price)) + 
  geom_point(alpha = .5) + 
  scale_y_log10()

#SKIP
# try nonlinear term Latitude
ames %>% ggplot(aes(x = Latitude, y = Sale_Price)) + 
  geom_point(alpha = .5) + 
  scale_y_log10() +
  geom_smooth(method = "lm", 
    formula = y ~ splines::bs(x, 5), 
    se = FALSE)
```

## Data Splitting for FLU
**slide 9**

Before we go too far, we are going to split our data into training and test sets. In the case of the flu data, we are going to train on cases with known outcome and test on cases with unknown outcome.

```{r}
train_flu <- flu %>%
  filter(!is.na(outcome)) %>%
  select(-case_id)
train_flu

test_flu <- flu %>%
  filter(is.na(outcome)) %>%
  select(-case_id)
test_flu
```

## Data Splitting for Ames

#### But first, a note on reproducibility and `set.seed()`

When we train a model using resampling, that sampling is going to happen _pseudo_-randomly. Try running this function which generates five numbers from a random uniform distribution between 0 and 1.

```{r}
runif(5)
runif(5)
```

If you run that function over and over again, you'll get different results. But, we can set the random number seed generator with any value we choose, and we'll get the same result. Try setting the seed, drawing the random numbers, then re-setting the same seed, and re-running the `runif` function again. You should get identical results.

```{r}
set.seed(190131)
runif(5)
runif(5)
```

Eventually we are going to compare different models to each other, so we want to set the random number seed generator to the same value for each model so the same resamples are identical across models. We don't want the difference in model performance to be due to differences in bootstrapping or cross validation. With a huge sample size, this would not matter, but to be safe, let's set the seed.

Let's hold out 20% for testing and use the other 80% for training. To accomplish the split there is an easy function in the {caret} package `createDataPartition()`. If you need to stratify sampling into training and testing such that an equal proportion of say diseased and healthy make it into the training / testing sets, createDataPartition does this automatically. You can also specify stratification on predictors if needed.
```{r}
set.seed(190131)
inTrainAmes <- createDataPartition(ames$Sale_Price, p=0.8, list=FALSE)

ames_train <- ames[inTrainAmes, ]
ames_test <- ames[-inTrainAmes, ]
```

## Recipe for FLU
**slide 10**

So far, our flu data has the following variables:
- outcome: Death / Recovery / NA
- gender: m/f (needs conversion to dummy)
- age: numeric
- province: Shanghai / Other / Jiangsu / Zhejiang (needs conversion to dummy)
- hospital: numeric
- days_to_hosp: numeric
- days_to_outcome: numeric
- early_outcome: numeric

Let's write a recipe to handle preprocessing steps like making dummy variables and missing data imputation. Recipes are a nice method for creating the data frame of predictors for a model. They allow us to specify a sequence of steps that define how data should be handled. 

When we run regression models, R handles dummy coding internally without our intervention. But we need to be explicit here since some of the machine learning algorithms will not dummy code for us.

Second, this dataset has a lot of missing data points throughout. Most of the data mining algorithms we're going to use later can't handle missing data, so observations with any missing data are excluded from the model completely. If we have a large dataset and only a few missing values, it can be better to exclude them and proceed. But since we've already got a pretty low number of observations, we will impute missing values to maximize our use of the data we have.

There are lots of different imputation approaches. An overly simplistic method is simply a mean or median imputation -- you simply plug in the mean value for that column for the missing sample's value. This leaves the mean unchanged (good) but artificially decreases the variance (not good). 

We'll use bagged trees to impute our missing data. When we get to model fitting we will explain more about bagged trees.

```{r, flurecipe}
library(recipes)
flu_rec <- recipe(outcome ~ ., data = train_flu) %>%
  # Impute missings in predictors
  step_bagimpute(all_predictors(), seed_val = 190131) %>%
  
  # Create dummy variables for _all_ chr or factor predictor variables
  step_dummy(all_nominal(), -all_outcomes())

flu_rec
flu_rec$var_info
```

This creates the recipe for data processing (but does not execute it yet). We will use this recipe when we create models. If we want to see the prprocessing steps after they have been run on the data, the next step is to train the preprocessing on the training set using `prep`and then `bake` the recipe to apply it to the training and the test sets.

```{r, prepbake_flu}
flu_prep <- prep(flu_rec, training = train_flu)

#bake recipe
train_flu_prep <- bake(flu_prep, new_data = train_flu)
test_flu_prep  <- bake(flu_prep, new_data = test_flu)
```

## Recipe for Ames

For the Ames dataset, let's create a recipe for a somewhat simple model so that we can keep track of the preprocessing steps that we should take. We'll fit a model predicting sale price from longitude, latitude, and the neighborhood that each house is in. We already saw that the sales price should be log transformed. We also saw that the relationship between Longitude and Latitude should be modeled with nonlinear terms. Finally, we know that some of the neighborhoods have only a few observations so let's lump them together into an "other" category. Finally, we'll transform the nominal variables into dummy variables.

We should note that the order of the steps is important so we should lump the neighborhoods together before we create the dummy variables.

```{r, rec_ames1}
rec_ames <- recipe(Sale_Price ~ Longitude + Latitude + Neighborhood, data = ames_train) %>%
  step_log(Sale_Price, base = 10) %>%
    # Lump factor levels that occur in 
  # <= 5% of data as "other"
  step_other(Neighborhood, threshold = 0.05) %>%
  
  # Create dummy variables for _any_ factor variables
  step_dummy(all_nominal()) %>%
  step_scale(all_predictors())

rec_ames
```

Use prep to train the recipe on the data to result in a pre-processed training set.
```{r, rec_ames2}
rec_ames_prep <- prep(rec_ames, training = ames_train, verbose = TRUE)

rec_ames_prep
```

Now apply the preprocessing steps to the training and test sets using bake
```{r, rec_ames3}
ames_train_prep <- bake(rec_ames_prep, new_data = ames_train)
ames_test_prep <- bake(rec_ames_prep, new_data = ames_test)

#take a look at the baked training set
ames_train_prep
```

## EXERCISE 2
**YOUR TURN**

Look up the documentation for the recipes package and find how to add a step to eliminate any near zero-variance predictors. Create a recipe to predict Sale_Price ~ Longitude + Latitude + Neighborhood and add in this step. Investigate the effect on the Neighborhoods variables.

```{r,include=FALSE}
mod_rec_zero <- recipe(
  Sale_Price ~ Longitude + Latitude + Neighborhood, 
  data = ames_train
) %>%
  step_log(Sale_Price, base = 10) %>%
  step_dummy(all_nominal()) %>%
  step_nzv(starts_with("Neighborhood")) 

mod_rec_zero
mod_rec_zero_prep <- prep(mod_rec_zero, training = ames_train, verbose = TRUE)
ames_train_zero <- bake(mod_rec_zero_prep, new_data = ames_train)

# see which neighborhoods were kept
ames_train_zero %>%
  select(contains("Neighborhood"))
```

------

Review what we have done and then **Take a break**

Now that we have a recipe that will preprocess our data such that the training and testing sets are preprocessed in exactly the same way, we are ready to jump into the caret package to fit some models inside a resampling loop.

## The caret package
**slide 11**

We're going to use the **caret** package for building and testing predictive models using a variety of different data mining / ML algorithms. The package was published in JSS in 2008. Max Kuhn's [caret package vignette](https://cran.r-project.org/web/packages/caret/vignettes/caret.html) is a great resource and has [detailed e-book documentation](http://topepo.github.io/caret/) to accompany it. 

> Kuhn, M. (2008). Building Predictive Models in R Using the caret Package. _Journal of Statistical Software_, 28(5), 1 - 26. doi: <http://dx.doi.org/10.18637/jss.v028.i05>

The [**caret**](http://cran.r-project.org/web/packages/caret/index.html) package (short for **C**lassification **A**nd **RE**gression **T**raining) is a set of functions that streamline the process for creating and testing a wide variety of predictive models with different resampling approaches, as well as estimating variable importance from developed models. There are many different modeling functions in R spread across many different packages, and they all have different syntax for model training and/or prediction. The **caret** package provides a uniform interface the functions themselves, as well as a way to standardize common tasks (such parameter tuning and variable importance).

#### Model training

The `train` function from caret is used to:

- evaluate, using resampling, the effect of model tuning parameters on performance
- choose the "optimal" model across these parameters
- estimate model performance from a training set

#### Resampling 
**slides 12 - 15**

The default resampling scheme caret uses is the [bootstrap](https://en.wikipedia.org/wiki/Bootstrapping_(statistics)). Bootstrapping takes a random sample _with replacement_ from your data that's the same size of the original data. Samples might be selected more than once, and some aren't selected at all. On average, each sample has a ~63.2% chance of showing up at least once in a bootstrap sample. Some samples won't show up at all, and these _held out_ samples are the ones that are used for testing the performance of the trained model. You repeat this process many times (e.g., 25, 100, etc) to get an average performance estimate on unseen data. Here's what it looks like in practice.

> Note: The 632 rule comes from the fact that (1-e^-1)n = .632n so a sample with replacement will have 63% of the samples in it

Another popular approach is cross-validation. Here, a subset of your training data (e.g., 4/5ths, or 80%) is used for analysis, and the remaining 1/5th or 20% is used for assessment. You slide the cross-validation interval over and use the next 4/5ths for analysis and 1/5th for assessment. You do this again for all 5ths of the data. You can optionally repeat this process many times (_repeated cross-validation_) to get an average cross validation prediction accuracy for a given model and set of tuning parameters.

The `trainControl` option in the `train` function controls this, and you can learn more about this under the [Basic Parameter Tuning](http://topepo.github.io/caret/model-training-and-tuning.html#basic-parameter-tuning) section of the caret documentation.

#### Models available in caret

First you have to choose a specific type of model or algorithm. Currently there are **`r I(length(unique(caret::modelLookup()$model)))`** different algorithms implemented in caret. Caret provides the interface to the method, but you still need the external package installed. For example, we'll be fitting a Random Forest model, and for that we'll need the [randomForest](https://cran.r-project.org/package=randomForest) package installed. You can see all the methods that you can deploy by looking at the help for train.

```{r, eval=FALSE}
?train
?models
```

From here, click on the link to see the [available models](http://topepo.github.io/caret/available-models.html) or [models by tag](http://topepo.github.io/caret/train-models-by-tag.html). From here you can search for particular models by name. We're going to fit models using Random Forest and Elastic-Net Regularized Generalized Linear Models. These require the packages [randomForest](https://cran.r-project.org/package=randomForest) and [glmnet](https://cran.r-project.org/package=glmnet), respectively. 

Each of the models may have one or more _tuning parameters_ -- some value or option you can set to tweak how the algorithm develops. With random forest, we can set the $m_{\text{try}}$ option -- the algorithm will select $m_{\text{try}}$ number of predictors to attempt a split for classification.

That is, it sweeps through each possible parameter you can set for the particular type of model you choose, and uses resampling with your training data, fitting the model on a subset and testing on the assessment samples. In this way we can determine the best tuning parameters to use on the test set

## Modeling for FLU

First we'll set up the resampling procedure using `trainControl()`. I'll set up one round of 10-fold cross validation. Usually we recommend method = "repeatedcv" with repeats = 5 but because we don't want to spend the whole workshop waiting for repeated CV, we'll just do one round of 10-fold CV here.
```{r, cache=TRUE}
# Set the random number seed generator
?train
set.seed(190131)

# Create trainControl object to specify resampling
ctrl <- trainControl(
  # 10-fold cv
  method = "cv", 
  # Save the assessment predictions from the best model
  savePredictions = "final",
  # Log the progress of the tuning process
  verboseIter = TRUE
  )
```

#### Random Forest
**slide 17 & 18**

Let's fit a [random forest](https://en.wikipedia.org/wiki/Random_forest) model.

> **_A bit about random forests._** Random forests are an ensemble learning approach based on classification trees. The CART (classification and regression tree) method searches through all available predictors to try to find a value of a single variable that splits the data into two groups by minimizing the impurity of the outcome between the two groups. The process is repeated over and over again until a hierarchical (tree) structure is created. But trees don't have great performance (prediction accuracy) compared to other models. Small changes in the data can drastically affect the structure of the tree. 
> 
> Tree algorithms are improved by ensemble approaches - instead of growing a single tree, grow many trees and aggregate (majority vote or averaging) the predictions made by the ensemble. The random forest algorithm is essentially:
> 
> 1. From the training data of _n_ samples, draw a cross validation analysis sample of size _n_.
> 1. For each cv fold analysis sample, grow a classification tree, but with a small modification compared to the traditional algorithm: instead of selecting from all possible predictor variables to form a split, choose the best split among a ** randomly selected subset ** of $m_{\text{try}}$ predictors. Here, $m_{\text{try}}$ is the only tuning parameter. The trees are grown to their maximum size and not "pruned" back.
> 1. Repeat the steps above until a large number of trees is grown. 
> 1. Estimate the performance of the ensemble of trees using the "out-of-bag" samples, the assessment samples in the cross validation procedure 
> 1. Estimate the importance of each variable in the model by randomly permuting each predictor variable in testing on the out-of-bag samples. If a predictor is important, prediction accuracy will degrade. If the predictor isn't that helpful, performance doesn't deteriorate as much. 
> 
> Random forests are efficient compared to growing a single tree. For one, the RF algorithm only selects from $m_{\text{try}}$ predictors at each step, rather than all available predictors. Usually $m_{\text{try}}$ is by default somewhere close to the square root of the total number of available predictors, so the search is very fast. Second, while the traditional CART tree algorithm has to go through extensive cross-validation based pruning to avoid overfitting, the RF algorithm doesn't do any pruning at all. In fact, building an RF model _can_ be faster than building a single tree!

See the help for `?train` and click on the link therein to see what abbreviations correspond to which model. First set the random number seed generator to some number, e.g., 190318, that we'll use for all the models we make. Notice how when we call `train()` from the **caret** package using "rf" as the type of model, it automatically loads the **[randomForest](https://cran.r-project.org/package=randomForest)** package that you installed. If you didn't have it installed, it would probably ask you to install it first.

```{r, flu_rf, cache=TRUE, message=FALSE}
library(randomForest)
set.seed(190131)
# Fit a random forest model using the recipe for preprocessing, the trainControl object

#note: to hide iterations through train(), I wrapped it in capture.output
garbage <- capture.output(
  modrf <- train(flu_rec, data=train_flu, method="rf", trControl = ctrl))
# Take a look at the output
modrf
```

Take a look at what that tells us. It tells us it's fitting a Random Forest model using 77 samples, predicting a categorical outcome class (Death or Recover) based on 7 predictors. It's doing pre-processing steps from our recipe, and it's doing 10fold CV. 

Random Forest has a single tuning parameter, $m_{\text{try}}$ -- the algorithm selects $m_{\text{try}}$ number of predictors to at each split for classification when building a classification tree. The caret package investigated mtry of 2, 5, and 9, and then computed the average [accuracy](https://en.wikipedia.org/wiki/Sensitivity_and_specificity#Confusion_matrix) and [kappa](https://en.wikipedia.org/wiki/Cohen%27s_kappa) measures of performance averaged over the assessment sets. It identified that mtry = 2 provided the best performance and so it fit the rf model with that.

[Accuracy](https://en.wikipedia.org/wiki/Sensitivity_and_specificity#Confusion_matrix) is the number of true assignments to the correct class divided by the total number of samples.

[Kappa](https://en.wikipedia.org/wiki/Cohen%27s_kappa) takes into account the expected accuracy while considering chance agreement, and is useful for extremely imbalanced class distributions. For continuous outcomes, you can measure things like RMSE or correlation coefficients.

Caret provides a function for [assessing the importance of each variable](http://topepo.github.io/caret/variable-importance.html). The `varImp` function knows what kind of model was fitted and knows how to estimate variable importance. For Random Forest, it's an estimate of how much worse the prediction gets after randomly shuffling the values of each predictor variable in turn. A variable that's important will result in a much worse prediction than a variable that's not as meaningful.

```{r}
varImp(modrf, scale=TRUE)
```

You can also pass that whole thing to `plot()`, or wrap the statement in `plot()`, to see a graphical representation.

```{r}
varImp(modrf, scale=TRUE) %>% plot()
```
We can't see from this plot whether being older or younger is more risky. We SHOULD NOT then use this info as input into a linear model because of too many type I errors. You CAN use a new dataset to model these variables to predict flu outcome.

#### k-nearest neighbor
**slide 19**

[k-nearest neighbor](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) attempts to assign samples to their closest labeled neighbors in high-dimensional space. You'll need the **[kknn](https://cran.r-project.org/package=kknn)** package for this.

```{r, flu_kknn, cache=TRUE, message=FALSE}
library(kknn)
set.seed(190131)
#note: to hide iterations through train(), I wrapped it in capture.output
garbage <- capture.output(
  modknn <- train(flu_rec, data=train_flu, method="kknn", trControl = ctrl))

# Take a look at the output
modknn
```

Notice that we are trying k = 5, 7, and 9. And the best is where k = 9. If we want to fit the models trying more neighbors, I would suggest setting up a tuning grid. We can read about the parameters in the train help menu and the kknn help menus.

```{r, flugrid_kknn, cache=TRUE, message=FALSE}
#kknn needs tuning parameters kmax, distance, and kernel
knn_grid <- expand.grid(kmax = seq(8, 15, by = 1), distance = 2, kernel = c("rectangular"))

#note: to hide iterations through train(), I wrapped it in capture.output
garbage <- capture.output(
  modknn <- train(flu_rec, data=train_flu, method="kknn", trControl = ctrl, tuneGrid = knn_grid))

# Take a look at the output
modknn
```

This takes a little longer since we are fitting more models on each fold. 

k = 13 is the best

#### Model comparison: Random Forest vs K-Nearest Neighbors

Let's compare those two models. Because the random seed was set to the same number (190131), the CV resamples were identical across each model. Let's directly compare the results for the best models from each method.

```{r}
modsum <- resamples(list(knn = modknn, rf=modrf))
summary(modsum)
```

For this dataset, it appears that random forest is doing much better in terms of both accuracy and kappa.

## Predict for flu

Once we have a model trained it's fairly simple to predict the class of the unknown samples. Take a look at the unknown data again:

```{r, results="hide"}
test_flu
```

Now, since Random Forest worked best, let's use that model to predict the outcome!

```{r}
predict(modrf, newdata=test_flu) %>% head()
```

This gives you a vector of values that would be the outcome for the individuals in the `test_flu` dataset.

Alternatively, you could pass in `type="prob"` to get prediction probabilities instead of predicted classes. 
```{r}
predict(modrf, newdata=test_flu, type="prob") %>% head()
```

You could imagine creating a cut-off for the probability of Recover or Death so that if prob > 0.7 then we will decide "Recover" or "Death" and if the probability is not high enough we could conclude "we don't know". If the cost of wrong decision is high, we could develop a gray zone. 

Fill in data with predicted values

```{r}
test_flu %>%
  mutate(outcome=predict(modrf, newdata=test_flu)) %>% head()
```

We won't know if these are correct since they were missing (~70% will be correct -- but there was more missing in one of the provinces)

If we had the true outcomes for the unknown cases, we could have looked at the confusion matrix (a table of predicted outcome against true outcome) to compute our test-set accuracy.

## Modeling for Ames

We'll use the same resampling procedure that we used before, the 10-fold cross validation without repeats so that it will not take too long.

#### Elastic net regularized logistic regression
**slides 20 & 21**

[Elastic net regularization](https://en.wikipedia.org/wiki/Elastic_net_regularization) is a method that combines both the [lasso](https://en.wikipedia.org/wiki/Lasso_(statistics)) and [ridge](https://en.wikipedia.org/wiki/Tikhonov_regularization) methods of reguarizing a model. [Regularization](https://en.wikipedia.org/wiki/Regularization_(mathematics)) is a method for _penalizing_ a model as it gains complexity with more predictors in an attempt to avoid overfitting. You'll need the **[glmnet](https://cran.r-project.org/package=glmnet)** package for this.

Like a logistic regression with lots of variables. By regularizing the model, you penalize for adding more variables. Simple models are more generalizable. 

To penalize a model we use lasso or ridge regularization. 

Lasso (stands for Least Absolute Shrinkage and Selection Operator) penalizes the raw coefficients (can be zero, so it can do variable selection too). Lasso has a tendency to over-regularize. 

Ridge regularization penalizes the squared coefficients so it never makes the coefficient zero. 

Elastic net combines both of those to control for overfitting and make models more generalizable. 

```{r, ames_glmnet, cache=TRUE, message=FALSE}
library(glmnet)

set.seed(190131)

#note: to hide iterations through train(), I wrapped it in capture.output
garbage <- capture.output(
  modglmnet <- train(rec_ames, data=ames_train, method="glmnet", trControl = ctrl))
```

Alpha (between 0 and 1) tunes between LASSO and ridge (smaller alpha means more Ridge, larger alpha means more LASSO)

Lambda (amount of regularization). Larger lambda means more coefficients shrunk toward zero.

Let's check out the final model.
```{r}
modglmnet
varImp(modglmnet)
varImp(modglmnet) %>% plot()
```
Best tuning parameters are: alpha = 0.55, lambda = 0.00135

RMSE = 0.1462, R^2 = 0.3287 (pretty poor)

## EXERCISE 3

Create a tuning grid of alpha and lambda where alpha ranges seq(0, 1, by = .25) and where lambda ranges from 10^seq(-3, -1, length = 20). Did we see an improvement in the Root Mean Squared Error (smaller is better) or R^2 (larger is better?)

```{r, echo=FALSE, results="hide", cache = TRUE, message=FALSE, fig.keep="none", warning=FALSE}
glmn_grid <- expand.grid(alpha = seq(0, 1, by = .25), lambda = 10^seq(-3, -1, length = 20))

set.seed(190131)

modglmnet <- train(rec_ames, data=ames_train, method="glmnet", trControl = ctrl, tuneGrid = glmn_grid)

# best model: alpha = 0 and lambda = 0.005455595
modglmnet #RMSE = 0.14616 R^2 = 0.329
varImp(modglmnet) #looks same
varImp(modglmnet) %>% plot()
```

## Predict for Ames
Let's use our best modglmnet to predict the Sale_Price on the test_set
```{r}
preds <- predict(modglmnet, newdata = ames_test)
#What is the RMSE?
postResample(pred = preds, obs = log10(ames_test$Sale_Price))
```
Hmm, we see a better fit on the new data than on the training set! Rare

Attach predictions of log10 Sale Price to test dataframe
```{r}
ames_test <- cbind(ames_test, preds)
```

See a plot of observed `Sale_Price` against predicted
```{r}
ames_test %>%
  ggplot(aes(log10(Sale_Price), preds)) + 
  geom_point() + 
  geom_abline(col = "red")
```

We could probably achieve a lower RMSE and a higher R^2 if we created a model with nonlinear terms for Longitude and Latitude since we saw in our EDA that they have a non-linear relationship with log10(Sale_Price). Perhaps we should investigate a model using splines for Lat and Long. Additionally, a model just using Lat, Long and Neighborhood is likely not going to explain the Sale_Price of a home very well.

**slides 22 & 23**