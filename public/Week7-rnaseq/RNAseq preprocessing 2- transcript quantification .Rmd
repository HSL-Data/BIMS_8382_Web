---
title: "RNAseq preprocessing 2: transcript quantification"
author: "Katie Owsiany"
date: "3/12/2019"
output: html_document
---
[link back to the RNAseq lesson](https://bims8382.netlify.com/week7-rnaseq/rnaseq)

In the previous tutorial, we preprocessed RNAseq data from the UVA genomics core using a STAR alignment and featureCounts workflow. While there are many variations on this workflow, the most common alteration involves how we count the reads after alignment. featureCounts is simple, quick, and accurate for gene level observations. However, transcript-level counting can be more accurate and sensitive for different isoforms of a given gene, since these will come from the same gene locus but have differentially spliced transcripts generating our reads. We can do a more sensitive analysis with the package Stringtie and Ballgown. 

##6. Stringtie transcript quantification 
We will make a folder called "stringtie2" to hold the output of this program, since [alndir] is getting crowded. 

>bash-4.2$ cd $ALNDIR

>bash-4.2$ mkdir stringtie2

>bash-4.2$ cd $SCRIPTS

>bash-4.2$ nano Stringtie-slurm.sh

Script
```
!/bin/bash
#SBATCH --nodes=1
#SBATCH --ntasks=20
#SBATCH --mem=240gb
#SBATCH --time=800
#SBATCH -p standard
#SBATCH -A owens_rivanna

module load gcc/7.1.0

ALNDIR=$ALNDIR
ANNOTATION=$ANNOTATION
STRINGTIE=$STRINGTIE

find $ALNDIR -name "*star.Aligned.sortedByCoord.out.bam" | awk -F"/" '{print $NF}' | cut -f 1-3 -d "_" | parallel -j 1 "$STRINGTIE $ALNDIR/{} -v -p 12 -G $ANNOTATION -o $ALNDIR/stringtie/{}.out.gtf -l {}"
```
##7. Stringtie merge
To merge the transcripts into a non-redundant list, we will run Stringtie in merge mode. For this we require a text file "mergelist.txt" of our input. 

>bash-4.2$find $ALNDIR/stringtie -name "*.gtf" > $ALNDIR/stringtie2/mergelist.txt


Now we can use this in a new Stringtie-merge script.

>bash-4.2$ cd $SCRIPTS

>bash-4.2$ nano Stringtie-merge.sh

```
#!/bin/bash
#SBATCH --nodes=1
#SBATCH --ntasks=20
#SBATCH --mem=240gb
#SBATCH --time=800
#SBATCH -p standard
#SBATCH -A owens_rivanna

module load gcc/7.1.0

ALNDIR=$ALNDIR
ANNOTATION=$ANNOTATION
STRINGTIE=$STRINGTIE
EXPERIMENT=$EXPERIMENT

$STRINGTIE --merge -p 12 -G $ANNOTATION -o $ALNDIR/stringtie/$EXPERIMENT.stringtie.merged.gtf $ALNDIR/stringtie/mergelist.txt
```

>bash-4.2$ sbatch Stringtie-merge.sh


Now we will create a folder "ballgown" to hold the merged lists of abundance of each gene in each sample. 

>bash-4.2$ cd $ALNDIR

>bash-4.2$ mkdir ballgown

>bash-4.2$ cd $SCRIPTS

>bash-4.2$ nano Stringtie-abundance-slurm.sh

Script 
```
#!/bin/bash
#SBATCH --nodes=1
#SBATCH --ntasks=20
#SBATCH --mem=240gb
#SBATCH --time=800
#SBATCH -p standard
#SBATCH -A owens_rivanna

module load gcc/7.1.0

ALNDIR=$ALNDIR
ANNOTATION=$ANNOTATION
STRINGTIE=$STRINGTIE
EXPERIMENT=$EXPERIMENT

find $ALNDIR -name "*.star.Aligned.sortedByCoord.out.bam" | awk -F"/" '{print $NF}' | cut -f 1-3 -d "_" | parallel -j 1 "$STRINGTIE $ALNDIR/{} -e -p 12 -B -G $ALNDIR/stringtie/$EXPERIMENT.stringtie.merged.gtf -o $ALNDIR/ballgown/{}/{}.final.gtf -l {}
```

>bash-4.2$ sbatch Stringtie-abundance-slurm.sh

##8. Export count and column data to DESeq2 format
One more big hurdle to overcome before we can move back into comfortable R: formatting this aligned, counted, assigned, and merged file into DESeq2. We will do this with a python script called prepDE.py. This script can be altered to change what kind of output you get- for example, I have it set to give me gene names as the output instead of ENS transcript codes. This is a big code that can be examined/edited on your computer and uploaded to Rivanna using Globus. 

>bash-4.2$ nano prepDE.py

```
#!/usr/bin/env python2
import re, csv, sys, os, glob, warnings, itertools
from math import ceil
from optparse import OptionParser
from operator import itemgetter
#note that the gtf files in the sample folders have same # of lines, just different order(?)

parser=OptionParser(description='Generates two CSV files containing the count matrices for genes and transcripts, using the coverage values found in the output of `stringtie -e`')
parser.add_option('-i', '--input', '--in', default='ballgown', help="the parent directory of the sample sub-directories or a textfile listing the paths to GTF files [default: %default]")
parser.add_option('-g', default='gene_count_matrix.csv', help="where to output the gene count matrix [default: %default")
parser.add_option('-t', default='transcript_count_matrix.csv', help="where to output the transcript count matrix [default: %default]")
parser.add_option('-l', '--length', default=75, type='int', help="the average read length [default: %default]")
parser.add_option('-p', '--pattern', default=".", help="a regular expression that selects the sample subdirectories")
parser.add_option('-c', '--cluster', action="store_true", help="whether to cluster genes that overlap with different gene IDs, ignoring ones with geneID pattern (see below)")
parser.add_option('-s', '--string', default="MSTRG", help="if a different prefix is used for geneIDs assigned by StringTie [default: %default]")
parser.add_option('-k', '--key', default="prepG", help="if clustering, what prefix to use for geneIDs assigned by this script [default: %default]")
parser.add_option('--legend', default="legend.csv", help="if clustering, where to output the legend file mapping transcripts to assigned geneIDs [default: %default]")
(opts, args)=parser.parse_args()

samples = [] # List of tuples. If sample list, (first column, path). Else, (subdirectory name, path to gtf file in subdirectory)
if (os.path.isfile(opts.input)):
    # gtfList = True
    try:
        fin = open(opts.input, 'r')
        for line in fin:
            if line[0] != '#':
                lineLst = tuple(line.strip().split())
                if (len(lineLst) != 2):
                    print "Error: Text file with sample ID and path invalid (%s)" % (line.strip())
                    exit(1)
                if lineLst[0] in samples:
                    print "Error: Sample ID duplicated (%s)" % (lineLst[0])
                    exit(1)
                if not os.path.isfile(lineLst[1]):
                    print "Error: GTF file not found (%s)" % (lineLst[1])
                    exit(1)
                samples.append(lineLst)
    except IOError:
        print "Error: List of .gtf files, %s, doesn't exist" % (opts.input)
        exit(1)
else:
    # gtfList = False
    ## Check that opts.input directory exists
    if not os.path.isdir(opts.input):
      parser.print_help()
      print " "
      print "Error: sub-directory '%s' not found!" % (opts.input) 
      for line in fin:
            if line[0] != '#':
                lineLst = tuple(line.strip().split())
                if (len(lineLst) != 2):
                    print "Error: Text file with sample ID and path invalid (%s)" % (line.strip())
                    exit(1)
                if lineLst[0] in samples:
                    print "Error: Sample ID duplicated (%s)" % (lineLst[0])
                    exit(1)
                if not os.path.isfile(lineLst[1]):
                    print "Error: GTF file not found (%s)" % (lineLst[1])
                    exit(1)
                samples.append(lineLst)
    except IOError:
        print "Error: List of .gtf files, %s, doesn't exist" % (opts.input)
        exit(1)
else:
    # gtfList = False
    ## Check that opts.input directory exists
    if not os.path.isdir(opts.input):
      parser.print_help()
      print " "
      print "Error: sub-directory '%s' not found!" % (opts.input)
      sys.exit(1)

    #####
    ## Collect all samples file paths and if empty print help message and quit
    #####
    samples = [(i,glob.iglob(os.path.join(opts.input,i,"*.gtf")).next()) for i in next(os.walk(opts.input))[1] if re.search(opts.pattern,i)]

if len(samples) == 0:
  parser.print_help()
  print " "
  print "Error: no GTF files found under ./%s !" % (opts.input)
  sys.exit(1)

RE_GENE_ID=re.compile('gene_name "([^"]+)"')
RE_GENE_NAME=re.compile('gene_name "([^"]+)"')
RE_TRANSCRIPT_ID=re.compile('transcript_id "([^"]+)"')
RE_COVERAGE=re.compile('cov "([\-\+\d\.]+)"')
RE_STRING=re.compile(re.escape(opts.string))

#####
## Sort the sample names by the sample ID
#####
samples.sort()
#####
## Checks whether a given row is a transcript
## other options: ex. exon, transcript, mRNA, 5'UTR
#####
def is_transcript(x):
  return len(x)>2 and x[2]=="transcript"

def getGeneID(s, ctg, tid):
  r=RE_GENE_ID.search(s)
  if r: return r.group(1)
  r=RE_GENE_NAME.search(s)
  if r: return ctg+'|'+r.group(1)
  return tid

def getCov(s):
  r=RE_COVERAGE.search(s)
  if r:
    v=float(r.group(1))
    if v<0.0: v=0.0
    return v
  return 0.0

def is_overlap(x,y): #NEEDS TO BE INTS!
  return x[0]<=y[1] and y[0]<=x[1]


def t_overlap(t1, t2): #from badGenes: chromosome, strand, cluster, start, end, (e1start, e1end)...
    if t1[0] != t2[0] or t1[1] != t2[1] or t1[5]<t2[4]: return False
    for i in range(6, len(t1)):
        for j in range(6, len(t2)):
            if is_overlap(t1[i], t2[j]): return True
    return False

## Average Readlength
read_len=opts.length

## Variables/Matrices to store t/g_counts
t_count_matrix, g_count_matrix=[],[]

##Get ready for clustering, stuff is once for all samples##
geneIDs={} #key=transcript, value=cluster/gene_id


## For each of the sorted sample paths
for s in samples:
    badGenes=[] #list of bad genes (just ones that aren't MSTRG)
    try:
        ## opts.input = parent directory of sample subdirectories
        ## s = sample currently iterating through
        ## os.path.join(opts.input,s,"*.gtf") path to current sample's GTF
        ## split = list of lists: [[chromosome, ...],...]

        #with open(glob.iglob(os.path.join(opts.input,s,"*.gtf")).next()) as f:
        #    split=[l.split('\t') for l in f.readlines()]
#        if not gtfList:
#            f = open(glob.iglob(os.path.join(opts.input,s[1],"*.gtf")).next())

f = open(s[1])
        with open(s[1]) as f:
            split=[l.split('\t') for l in f.readlines()]

        ## i = numLine; v = corresponding i-th GTF row
        for i,v in enumerate(split):
            if is_transcript(v):
                t_id=RE_TRANSCRIPT_ID.search(v[len(v)-1]).group(1)
                try:
                  g_id=getGeneID(v[len(v)-1], v[0], t_id)
                except:
                  print "Problem at line:\n:%s\n" % (v)
                  print "i='%s', len(v)=%s" % (i, len(v));
                  sys.exit(1)
                geneIDs.setdefault(t_id, g_id)
                if not RE_STRING.match(g_id):
                    badGenes.append([v[0],v[6], t_id, g_id, min(int(v[3]),int(v[4])), max(int(v[3]),int(v[4]))]) #chromosome, strand, cluster/transcript id, start, end
                    j=i+1
                    while j<len(split) and split[j][2]=="exon":
                        badGenes[len(badGenes)-1].append((min(int(split[j][3]), int(split[j][4])), max(int(split[j][3]), int(split[j][4]))))
                        j+=1

    except StopIteration:
        warnings.warn("Didn't get a GTF in that directory. Looking in another...")

    else: #we found the "bad" genes!
        break
        ##THE CLUSTERING BEGINS!##
if opts.cluster and len(badGenes)>0:
    clusters=[] #lists of lists (could be sets) or something of transcripts
    badGenes.sort(key=itemgetter(3)) #sort by start coord...?
    i=0
    while i<len(badGenes): #rather un-pythonic
        temp_cluster=[badGenes[i]]

        k=0
	while k<len(temp_cluster):
            j=i+1
            while j<len(badGenes):
                if t_overlap(temp_cluster[k], badGenes[j]):
                    temp_cluster.append(badGenes[j])
                    del badGenes[j]
                else:
                    j+=1
            k+=1
        if len(temp_cluster)>1:
            clusters.append([t[2] for t in temp_cluster])
        i+=1

    print len(clusters)

    for c in clusters:
        c.sort()

    clusters.sort(key=itemgetter(0))
    legend=[]
    for u,c in enumerate(clusters):
        my_ID=opts.key+str((u+1))
        legend.append(list(itertools.chain.from_iterable([[my_ID],c]))) #my_ID, clustered transcript IDs
        for t in c:
            geneIDs[t]=my_ID
##            geneIDs[t]="|".join(c) #duct-tape transcript IDs together, disregarding ref_gene_names and things like that
 with open(opts.legend, 'w') as l_file:
        my_writer=csv.writer(l_file)
        my_writer.writerows(legend)

geneDict={} #key=gene/cluster, value=dictionary with key=sample, value=summed counts
t_dict={}
for q, s in enumerate(samples):
    print q, s[0]

    try:
        #with open(glob.iglob(os.path.join(opts.input,s,"*.gtf")).next()) as f: #grabs first .gtf file it finds inside the sample subdirectory
#        if not gtfList:
#            f = open(glob.iglob(os.path.join(opts.input,s[1],"*.gtf")).next())
#        else:
	f = open(s[1])

            # s = s.split('/')[len(s.split('/')) - 1].split('.gtf')[0].split('_')[0]
            # s = sample_IDs[q]

##        split=[t[:len(t)-1]+t[len(t)-1].split(";") for t in split]
##        split=[t[:len(t)-1] for t in split] #eliminate '\n' at end
##        split=[[e.lstrip() for e in t] for t in split]
        #should consider making stuff into dictionaries, maybe each split line

##            transcriptList=[]
        transcript_len=0
        for l in f:
            if l.startswith("#"):
                continue
            v=l.split('\t')
            if v[2]=="transcript":
                if transcript_len>0:
##                        transcriptList.append((g_id, t_id, int(ceil(coverage*transcript_len/read_len))))
                    t_dict.setdefault(t_id, {})
                    t_dict[t_id].setdefault(s[0], int(ceil(coverage*transcript_len/read_len)))
                t_id=RE_TRANSCRIPT_ID.search(v[len(v)-1]).group(1)
                #g_id=RE_GENE_ID.search(v[len(v)-1]).group(1)
                g_id=getGeneID(v[len(v)-1], v[0], t_id)
                #coverage=float(RE_COVERAGE.search(v[len(v)-1]).group(1))
                coverage=getCov(v[len(v)-1])
                transcript_len=0
            if v[2]=="exon":
                transcript_len+=int(v[4])-int(v[3])+1 #because end coordinates are inclusive in GTF

##            transcriptList.append((g_id, t_id, int(ceil(coverage*transcript_len/read_len))))
        t_dict.setdefault(t_id, {})
        t_dict[t_id].setdefault(s[0], int(ceil(coverage*transcript_len/read_len)))

    except StopIteration:
#        if not gtfList:
#            warnings.warn("No GTF file found in " + os.path.join(opts.input,s[1]))
#        else:
	warnings.warn("No GTF file found in " + s[1])


##        transcriptList.sort(key=lambda bla: bla[1]) #gene_id

    for i,v in t_dict.iteritems():
##        print i,v
        geneDict.setdefault(geneIDs[i],{}) #gene_id
        geneDict[geneIDs[i]].setdefault(s[0],0)
        geneDict[geneIDs[i]][s[0]]+=v[s[0]]
        
with open(opts.t, 'w') as csvfile:
    my_writer = csv.DictWriter(csvfile, fieldnames = ["transcript_id"] + [x for x,y in samples])
    my_writer.writerow(dict((fn,fn) for fn in my_writer.fieldnames))
    for i in t_dict:
        t_dict[i]["transcript_id"] = i
        my_writer.writerow(t_dict[i])

with open(opts.g, 'w') as csvfile:
    my_writer = csv.DictWriter(csvfile, fieldnames = ["gene_id"] + [x for x,y in samples])
##    my_writer.writerow([""]+samples)
##    my_writer.writerows(geneDict)
    my_writer.writerow(dict((fn,fn) for fn in my_writer.fieldnames))
    for i in geneDict:
        geneDict[i]["gene_id"] = i #add gene_id to row
        my_writer.writerow(geneDict[i])
```

For this we also need to provide the program with a list of the names we want to give each sample file. You can make this in Rivanna with nano or create a text file on your computer and upload it. 

>bash-4.2$ cd $ALNDIR/ballgown

>bash-4.2$ nano sample_list.txt

```
GreenAth_S5_L001	/scratch/ko8xh/[SMARTseq]/[alndir]/ballgown/GreenAth_S5_L001.2.star.Aligned.sortedByCoord.out.bam/GreenAth_S5_L001.2.star.Aligned.sortedByCoord.out.bam.final.gtf	
GreenAth_S5_L002	/scratch/ko8xh/[SMARTseq]/[alndir]/ballgown/GreenAth_S5_L002.2.star.Aligned.sortedByCoord.out.bam/GreenAth_S5_L002.2.star.Aligned.sortedByCoord.out.bam.final.gtf
GreenAth_S5_L003	/scratch/ko8xh/[SMARTseq]/[alndir]/ballgown/GreenAth_S5_L003.2.star.Aligned.sortedByCoord.out.bam/GreenAth_S5_L003.2.star.Aligned.sortedByCoord.out.bam.final.gtf
GreenAth_S5_L004	/scratch/ko8xh/[SMARTseq]/[alndir]/ballgown/GreenAth_S5_L004.2.star.Aligned.sortedByCoord.out.bam/GreenAth_S5_L004.2.star.Aligned.sortedByCoord.out.bam.final.gtf
Greenburn_S1_L001	/scratch/ko8xh/[SMARTseq]/[alndir]/ballgown/Greenburn_S1_L001.2.star.Aligned.sortedByCoord.out.bam/Greenburn_S1_L001.2.star.Aligned.sortedByCoord.out.bam.final.gtf
Greenburn_S1_L002	/scratch/ko8xh/[SMARTseq]/[alndir]/ballgown/Greenburn_S1_L002.2.star.Aligned.sortedByCoord.out.bam/Greenburn_S1_L002.2.star.Aligned.sortedByCoord.out.bam.final.gtf
Greenburn_S1_L003	/scratch/ko8xh/[SMARTseq]/[alndir]/ballgown/Greenburn_S1_L003.2.star.Aligned.sortedByCoord.out.bam/Greenburn_S1_L003.2.star.Aligned.sortedByCoord.out.bam.final.gtf
Greenburn_S1_L004	/scratch/ko8xh/[SMARTseq]/[alndir]/ballgown/Greenburn_S1_L004.2.star.Aligned.sortedByCoord.out.bam/Greenburn_S1_L004.2.star.Aligned.sortedByCoord.out.bam.final.gtf
TomB_S3_L001	/scratch/ko8xh/[SMARTseq]/[alndir]/ballgown/TomB_S3_L001.2.star.Aligned.sortedByCoord.out.bam/TomB_S3_L001.2.star.Aligned.sortedByCoord.out.bam.final.gtf
TomB_S3_L002	/scratch/ko8xh/[SMARTseq]/[alndir]/ballgown/TomB_S3_L002.2.star.Aligned.sortedByCoord.out.bam/TomB_S3_L002.2.star.Aligned.sortedByCoord.out.bam.final.gtf
TomB_S3_L003	/scratch/ko8xh/[SMARTseq]/[alndir]/ballgown/TomB_S3_L003.2.star.Aligned.sortedByCoord.out.bam/TomB_S3_L003.2.star.Aligned.sortedByCoord.out.bam.final.gtf
TomB_S3_L004	/scratch/ko8xh/[SMARTseq]/[alndir]/ballgown/TomB_S3_L004.2.star.Aligned.sortedByCoord.out.bam/TomB_S3_L004.2.star.Aligned.sortedByCoord.out.bam.final.gtf
TomUB_S2_L001	/scratch/ko8xh/[SMARTseq]/[alndir]/ballgown/TomUB_S2_L001.2.star.Aligned.sortedByCoord.out.bam/TomUB_S2_L001.2.star.Aligned.sortedByCoord.out.bam.final.gtf
TomUB_S2_L002	/scratch/ko8xh/[SMARTseq]/[alndir]/ballgown/TomUB_S2_L002.2.star.Aligned.sortedByCoord.out.bam/TomUB_S2_L002.2.star.Aligned.sortedByCoord.out.bam.final.gtf
TomUB_S2_L003	/scratch/ko8xh/[SMARTseq]/[alndir]/ballgown/TomUB_S2_L003.2.star.Aligned.sortedByCoord.out.bam/TomUB_S2_L003.2.star.Aligned.sortedByCoord.out.bam.final.gtf
TomUB_S2_L004	/scratch/ko8xh/[SMARTseq]/[alndir]/ballgown/TomUB_S2_L004.2.star.Aligned.sortedByCoord.out.bam/TomUB_S2_L004.2.star.Aligned.sortedByCoord.out.bam.final.gtf
```
Ok so now we have our final script, which loads a module called anaconda2 to allow us to use python code, and then calls prepDE.py to convert our data into final transcript and gene count matrices for DESeq2.

>bash-4.2$ nano Stringtie2desq2-slurm.sh

```
#!/bin/bash
#SBATCH --nodes=1
#SBATCH --ntasks=20
#SBATCH --mem=240gb
#SBATCH --time=800
#SBATCH -p standard
#SBATCH -A owens_rivanna

module load anaconda2

ALNDIR=$ALNDIR
SCRIPTS=$SCRIPTS

python $SCRIPTS/prepDE.py -i $ALNDIR/ballgown/sample_list.txt -g $ALNDIR/stringtietodeseq2/RNAseq-genes.csv -t $ALNDIR/stringtietodeseq2/RNAseq-transcripts.csv
```


>bash-4.2$ sbatch Stringtie2deseq2-slurm.sh

This gives us final output files called RNAseq-genes.csv and RNAseq-transcripts.csv. These files are count matricies that can be downloaded and read into R using read_csv.

